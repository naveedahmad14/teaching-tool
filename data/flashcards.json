[
  {
    "id": "linear",
    "name": "Linear Search",
    "lessonLink": "/lesson/linear",
    "cards": [
      {
        "question": "In what order does Linear Search examine elements?",
        "tip": "Sequential = in order, like reading a book page by page from start to finish!"
      },
      {
        "question": "What is Linear Search's best-case time complexity?",
        "tip": "Best case = lucky first try! Found at index 0 = constant time O(1)."
      },
      {
        "question": "Does Linear Search require the array to be sorted?",
        "tip": "Linear Search = no prerequisites! Works on any data order, unlike Binary Search."
      },
      {
        "question": "What does Linear Search return when the element is not found?",
        "tip": "Not found = -1. Valid indices start at 0, so -1 clearly means 'not present'!"
      },
      {
        "question": "What is the space complexity of Linear Search?",
        "tip": "Just one counter variable needed! No extra arrays or recursion = O(1) space."
      },
      {
        "question": "In an unsorted array of n elements, what's the average number of comparisons Linear Search makes to find an element that exists?",
        "tip": "Average case = middle of array! For 10 elements: (10+1)/2 = 5.5 comparisons."
      },
      {
        "question": "What technique can optimize Linear Search for frequently accessed elements?",
        "tip": "Popular items move forward! Like organizing your desk - frequently used items on top."
      },
      {
        "question": "When is Linear Search more efficient than Binary Search?",
        "tip": "Small data = simple wins! Binary Search overhead not worth it for 5 elements."
      },
      {
        "question": "What is the worst-case scenario for Linear Search?",
        "tip": "Worst case = check everything! Last position or not found = maximum n comparisons."
      },
      {
        "question": "What optimization uses a 'sentinel' value in Linear Search?",
        "tip": "Sentinel = guaranteed to find! Place target at end → no bound check needed in loop."
      },
      {
        "question": "Given an unsorted array where 70% of searches hit the same 10% of elements, what's the optimal Linear Search strategy?",
        "tip": "Skewed access = self-organizing! Popular items bubble to front organically."
      },
      {
        "question": "What is the expected number of comparisons to find an element in an array of n elements, assuming uniform probability?",
        "tip": "Expected value = weighted average! Success cost + Failure cost, each weighted by probability."
      },
      {
        "question": "In a circular array (where last element connects to first), how does Linear Search complexity change?",
        "tip": "Circular = same total elements! May wrap around, but still check at most n items."
      },
      {
        "question": "For linked lists, why is Linear Search often preferred over Binary Search?",
        "tip": "No random access? Binary Search dies! Need O(1) middle access for efficiency."
      },
      {
        "question": "What parallel speedup can Linear Search theoretically achieve with k processors?",
        "tip": "Perfectly parallelizable! Divide array → k processors → k-times faster. Ideal speedup!"
      }
    ]
  },
  {
    "id": "binary",
    "name": "Binary Search",
    "lessonLink": "/lesson/binary",
    "cards": [
      {
        "question": "What is the primary requirement for Binary Search to work correctly?",
        "tip": "Sorted = foundation! Binary Search logic depends entirely on order to eliminate halves."
      },
      {
        "question": "How much of the search space does Binary Search eliminate with each comparison?",
        "tip": "Halving is the magic! Each step: n → n/2 → n/4 → n/8 → ... until found."
      },
      {
        "question": "What is Binary Search's time complexity?",
        "tip": "Log n = halving power! 1000 elements → only ~10 comparisons. Exponentially faster than O(n)!"
      },
      {
        "question": "Which index formula avoids integer overflow when calculating the middle?",
        "tip": "Overflow prevention! Subtract first, then divide. Keeps intermediate values small."
      },
      {
        "question": "What is Binary Search's space complexity for the iterative version?",
        "tip": "Iterative = constant space! Just 3 pointers. Recursive = log n stack frames."
      },
      {
        "question": "For an array of size 1,000,000, what's the maximum number of comparisons Binary Search makes?",
        "tip": "Million elements? Only 20 checks! 2²⁰ ≈ 1 million. Log scales incredibly well!"
      },
      {
        "question": "How can Binary Search find the first occurrence of a duplicate element?",
        "tip": "Found it? Keep going left! Don't stop at first match - may be more to the left."
      },
      {
        "question": "What happens if you use Binary Search on an unsorted array?",
        "tip": "Undefined behavior! May work sometimes, fail others. Never trust Binary Search on unsorted data."
      },
      {
        "question": "Why is Binary Search inefficient for linked lists?",
        "tip": "No random access = Binary Search fails! Can't jump to middle efficiently in linked lists."
      },
      {
        "question": "What's the loop termination condition in Binary Search?",
        "tip": "left <= right ensures we check the last element! left < right would skip single-element case."
      },
      {
        "question": "How do you search in a rotated sorted array [4,5,6,7,0,1,2]?",
        "tip": "One half always sorted! Check which half, determine if target in range, recurse accordingly."
      },
      {
        "question": "What's the tight lower bound for comparison-based searching in a sorted array?",
        "tip": "Binary Search is optimal! Can't do better than log n with comparisons on sorted data."
      },
      {
        "question": "How can Binary Search find the square root of n (without using sqrt)?",
        "tip": "Binary Search = not just arrays! Works on any monotonic function, including f(x) = x²."
      },
      {
        "question": "What is exponential search and when is it better than Binary Search?",
        "tip": "Near start? Exponential wins! Jump exponentially, then binary search the small range found."
      },
      {
        "question": "In a sorted matrix (rows and columns sorted), what's optimal search complexity?",
        "tip": "Top-right corner = decision point! Smaller→left, Larger→down. Eliminates row or column each step!"
      }
    ]
  },
  {
    "id": "bubble",
    "name": "Bubble Sort",
    "lessonLink": "/lesson/bubble",
    "cards": [
      {
        "question": "What does Bubble Sort compare in each step?",
        "tip": "Think about how bubbles rise in water - they move up one position at a time by comparing with their immediate neighbor!"
      },
      {
        "question": "After the first complete pass through an array, where is the largest element?",
        "tip": "Visualize: [5,2,8,1] → After pass 1, 8 must be at the end: [?,?,?,8]"
      },
      {
        "question": "What is the space complexity of Bubble Sort?",
        "tip": "Remember: In-place algorithms use constant extra space. We're just swapping elements within the original array!"
      },
      {
        "question": "When would Bubble Sort perform in O(n) time?",
        "tip": "Key insight: If we make a complete pass with NO swaps, the array must be sorted!"
      },
      {
        "question": "Is Bubble Sort a stable sorting algorithm?",
        "tip": "Think: [3a, 5, 3b] → We never swap 3a and 3b because they're equal, so their order is preserved!"
      },
      {
        "question": "How many passes are needed to sort an array of n elements in the worst case?",
        "tip": "For 5 elements: Pass 1→largest at end, Pass 2→2nd largest placed, Pass 3→3rd placed, Pass 4→done (5th auto-placed)!"
      },
      {
        "question": "What happens if you modify Bubble Sort to swap when arr[j] >= arr[j+1] instead of arr[j] > arr[j+1]?",
        "tip": "Stability depends on NOT swapping equal elements. >= would swap them unnecessarily!"
      },
      {
        "question": "In the optimized Bubble Sort, what does the 'swapped' flag detect?",
        "tip": "No swaps in a full pass = array is sorted! This optimization saves unnecessary comparisons."
      },
      {
        "question": "After k passes of Bubble Sort, how many elements are guaranteed to be in their final sorted positions?",
        "tip": "Pattern: Pass 1→1 sorted, Pass 2→2 sorted, Pass k→k sorted (all at the end)!"
      },
      {
        "question": "Why is Bubble Sort's average case O(n²)?",
        "tip": "Nested loops = multiply! Outer n × Inner n = n² time complexity."
      },
      {
        "question": "What is the exact number of comparisons Bubble Sort makes in the worst case for an array of size n?",
        "tip": "It's the sum of first (n-1) natural numbers! For n=5: 4+3+2+1 = 10 = 5(4)/2"
      },
      {
        "question": "Consider Bubble Sort on [5,1,4,2,8]. After 2 complete passes, what's the array state?",
        "tip": "Trace it step by step! Each pass moves one more element to its final position at the end."
      },
      {
        "question": "What technique could reduce Bubble Sort's worst-case swaps (though not comparisons)?",
        "tip": "Smart optimization: If last swap was at position k, everything after k is sorted!"
      },
      {
        "question": "Which statement about Bubble Sort's adaptive behavior is TRUE?",
        "tip": "Adaptive = performs better on partially sorted data. Without optimization, Bubble Sort doesn't detect this!"
      },
      {
        "question": "Why isn't Bubble Sort used in practice despite being simple?",
        "tip": "Simplicity ≠ Efficiency. Modern CPUs favor algorithms with good cache locality and fewer operations!"
      }
    ]
  },
  {
    "id": "merge",
    "name": "Merge Sort",
    "lessonLink": "/lesson/merge",
    "cards": [
      {
        "question": "What algorithmic paradigm does Merge Sort follow?",
        "tip": "Think: Divide (split array) → Conquer (sort halves) → Combine (merge sorted halves)!"
      },
      {
        "question": "What is Merge Sort's time complexity in ALL cases?",
        "tip": "Guaranteed performance! Unlike Quick Sort's O(n²) worst case, Merge Sort is ALWAYS O(n log n)!"
      },
      {
        "question": "What is the space complexity of standard Merge Sort?",
        "tip": "Trade-off: Guaranteed O(n log n) time BUT needs O(n) extra space for merging!"
      },
      {
        "question": "Is Merge Sort a stable sorting algorithm?",
        "tip": "Stable means: [3a, 5, 3b] → [3a, 3b, 5]. The two 3's stay in their original order!"
      },
      {
        "question": "How many times is each element copied during Merge Sort?",
        "tip": "Tree height = log n levels. Each level touches every element once = log n copies per element!"
      },
      {
        "question": "What is the height of Merge Sort's recursion tree for an array of size n?",
        "tip": "Halving pattern: 16→8→4→2→1 is 4 levels = log₂(16). Each level processes all n elements!"
      },
      {
        "question": "In the merge operation, what's the maximum number of comparisons needed to merge two sorted arrays of total size n?",
        "tip": "Think: Last element needs no comparison - it's automatically placed when one array empties!"
      },
      {
        "question": "When does Merge Sort perform better than Quick Sort?",
        "tip": "Merge Sort = consistency. Quick Sort = speed (usually). Choose based on priorities!"
      },
      {
        "question": "What optimization reduces Merge Sort's overhead on small subarrays?",
        "tip": "Small arrays: Insertion Sort wins! Large arrays: Merge Sort wins! Combine them for best results."
      },
      {
        "question": "How many recursive calls does Merge Sort make for an array of size n?",
        "tip": "Binary tree with n leaves has n-1 internal nodes. Total = n + (n-1) = 2n-1 calls!"
      },
      {
        "question": "Given [38, 27, 43, 3], what's the state after the FIRST complete merge?",
        "tip": "Trace bottom-up! Singles→pairs→final. Question catches you between merge levels!"
      },
      {
        "question": "What's the exact number of comparisons Merge Sort makes in the worst case for an array of size 8?",
        "tip": "Count merge by merge! Each merge of size k needs k-1 comparisons worst case."
      },
      {
        "question": "How can Merge Sort be implemented with O(1) space for linked lists?",
        "tip": "Arrays need temp space, but linked lists? Just rewire pointers! O(1) space achieved!"
      },
      {
        "question": "In external sorting with Merge Sort, what determines the number of passes?",
        "tip": "More merge ways (k) = fewer passes! Trade memory for speed in external sorting."
      },
      {
        "question": "Why is Merge Sort preferred for sorting data in tape drives?",
        "tip": "Sequential access only? Merge Sort shines! Quick Sort needs random jumping = bad for tapes."
      }
    ]
  },
  {
    "id": "quick",
    "name": "Quick Sort",
    "lessonLink": "/lesson/quick",
    "cards": [
      {
        "question": "What is the pivot in Quick Sort?",
        "tip": "Think: Pivot is the 'decision point' - everything gets sorted relative to it!"
      },
      {
        "question": "What happens to the pivot after the partition step?",
        "tip": "Key insight: After one partition, the pivot is DONE - it's in its correct spot forever!"
      },
      {
        "question": "What is Quick Sort's average time complexity?",
        "tip": "Remember: Divide (log n levels) × Conquer (n work per level) = O(n log n)!"
      },
      {
        "question": "Is Quick Sort an in-place sorting algorithm?",
        "tip": "In-place = swap within array, no extra array needed (just call stack for recursion)!"
      },
      {
        "question": "Is Quick Sort a stable sorting algorithm?",
        "tip": "Quick Sort = Fast but NOT stable. Merge Sort = Stable but needs extra space!"
      },
      {
        "question": "What causes Quick Sort's worst-case O(n²) performance?",
        "tip": "Visualize: Unbalanced partitions (1 vs n-1) create n levels instead of log n levels!"
      },
      {
        "question": "How does randomized Quick Sort improve performance?",
        "tip": "Random pivot = insurance against bad inputs! Can't predict worst case if pivot is random."
      },
      {
        "question": "What is Quick Sort's space complexity?",
        "tip": "Space = recursion depth! Balanced tree → log n depth. Unbalanced → n depth."
      },
      {
        "question": "In the partition step, what does the pointer 'i' track?",
        "tip": "Think: i = 'insertion point' for next small element. Everything before i is small!"
      },
      {
        "question": "Why is Quick Sort often faster than Merge Sort in practice?",
        "tip": "Cache locality matters! Sequential access (Quick Sort) beats scattered access (Merge Sort)."
      },
      {
        "question": "For array [3, 7, 8, 5, 2, 1, 9] with last element as pivot, what's the array after first partition?",
        "tip": "Trace step by step! i tracks boundary, j scans, swap when arr[j]<pivot. Practice makes perfect!"
      },
      {
        "question": "What optimization technique prevents stack overflow in Quick Sort?",
        "tip": "Smaller first = guarantees log n recursion depth! Larger partition handled iteratively."
      },
      {
        "question": "What is the median-of-three pivot selection strategy?",
        "tip": "Three candidates → pick middle value = better than worst case, faster than true median!"
      },
      {
        "question": "Why is Quick Sort preferred over Merge Sort for arrays?",
        "tip": "CPU cache loves Quick Sort! In-place + sequential access = fast in practice."
      },
      {
        "question": "What is the three-way partitioning optimization for Quick Sort?",
        "tip": "Many duplicates? Three-way saves the day! <pivot | =pivot | >pivot → skip equal elements!"
      }
    ]
  },
  {
    "id": "twopointers",
    "name": "Two Pointers",
    "lessonLink": "/lesson/twopointers",
    "cards": [
      {
        "question": "In the two-pointer remove-duplicates technique, what must be true about the array?",
        "tip": "Sorted → duplicates are next to each other. Read scans, write marks the end of the unique region."
      },
      {
        "question": "What does the 'write' pointer represent?",
        "tip": "write = 'unique region ends here'. We extend it by writing the next unique value at write+1."
      },
      {
        "question": "What is the time complexity of the two-pointer in-place remove-duplicates algorithm?",
        "tip": "One pass, constant work per element → O(n). No nested loops."
      },
      {
        "question": "What is the space complexity of the in-place two-pointer remove-duplicates?",
        "tip": "In-place = no extra array. Just two indices and the original array."
      },
      {
        "question": "After the algorithm finishes, what does 'write + 1' represent?",
        "tip": "Unique region is [0..write], so count = write + 1."
      },
      {
        "question": "For array [1,1,2,2,3], after the algorithm, what is the value of write and the effective length?",
        "tip": "Trace: read advances, we only copy when value changes. Count unique values = write+1."
      },
      {
        "question": "Why do we start with write=0 and read=1 (not read=0)?",
        "tip": "First element is always 'unique' so far. We only need to decide for indices 1..n-1."
      },
      {
        "question": "Can we use the same two-pointer idea for an unsorted array to remove duplicates?",
        "tip": "Unsorted → can't tell if value is duplicate without extra storage or sorting."
      },
      {
        "question": "What happens if the array is empty?",
        "tip": "Edge case: empty input → return 0."
      },
      {
        "question": "In the loop, when do we advance write and copy?",
        "tip": "New unique value ⇔ arr[read] != arr[write]. Then extend unique region."
      },
      {
        "question": "What is the minimum number of comparisons for the two-pointer remove-duplicates on n elements?",
        "tip": "One comparison per read step. read runs from 1 to n-1 → n-1 comparisons."
      },
      {
        "question": "If we wanted to remove duplicates and also return the new array (not just length), would space stay O(1)?",
        "tip": "In-place = modify given array, return length. New array = extra O(n) for output."
      },
      {
        "question": "How does the two-pointer method relate to the 'partition' idea in Quick Sort?",
        "tip": "Pattern: one pointer scans, one marks boundary. Reuse this in many in-place problems."
      },
      {
        "question": "For [1,1,1,1,1], how many times do we copy (assign to arr[write])?",
        "tip": "All same → no new unique value ever → write stays 0, zero copies."
      },
      {
        "question": "Which variant of 'two pointers' is used in remove-duplicates?",
        "tip": "Read/write = one scans, one marks 'valid result so far'. Different from opposite-end or fast/slow."
      }
    ]
  },
  {
    "id": "slidingwindow",
    "name": "Sliding Window",
    "lessonLink": "/lesson/slidingwindow",
    "cards": [
      {
        "question": "What does the 'sliding window' of size k represent in the max-sum problem?",
        "tip": "Contiguous = next to each other. Window = fixed-size block that moves."
      },
      {
        "question": "Why do we reuse the previous window sum instead of adding k elements each time?",
        "tip": "Slide = subtract one, add one. One pass → O(n)."
      },
      {
        "question": "When we slide the window right by one, which element leaves and which enters?",
        "tip": "Leave = arr[left], enter = arr[left+k]. Same as sliding a frame."
      },
      {
        "question": "What is the time complexity of the sliding-window max-sum algorithm?",
        "tip": "One initial sum + (n-k) slides of O(1) each = O(n)."
      },
      {
        "question": "What is the space complexity of the sliding-window max-sum?",
        "tip": "A few variables → O(1) extra space."
      },
      {
        "question": "For array [2,1,5,1,3,2] and k=3, what is the sum of the first window?",
        "tip": "First window = first k elements. 2+1+5 = 8."
      },
      {
        "question": "After the first window [2,1,5], when we slide right, which element leaves and which enters?",
        "tip": "Leave = left boundary, enter = right boundary of new window."
      },
      {
        "question": "How many possible windows of size k are there in an array of length n?",
        "tip": "Left index from 0 to n-k inclusive → n-k+1 windows."
      },
      {
        "question": "What if k > n?",
        "tip": "Edge case: k > n → no window. Return 0 or handle explicitly."
      },
      {
        "question": "Why is the sliding-window approach better than checking every subarray of size k?",
        "tip": "Reuse = constant work per step. Brute force = linear work per step."
      },
      {
        "question": "For [2,1,5,1,3,2], k=3, what is the maximum sum among all windows?",
        "tip": "Compute each window sum; track max. Answer 9."
      },
      {
        "question": "Can we use a sliding window for 'find minimum sum subarray of size k'?",
        "tip": "Same pattern: slide + maintain aggregate (min instead of max)."
      },
      {
        "question": "What type of sliding window is used for max-sum of size k?",
        "tip": "Fixed size k → one loop, slide by one. Variable size → expand/shrink."
      },
      {
        "question": "If the array has negative numbers, does the sliding-window max-sum algorithm still work?",
        "tip": "Sum is sum. Leave/enter update is arithmetic; negatives are fine."
      },
      {
        "question": "How many times do we add an element to the running sum over the entire algorithm?",
        "tip": "Each element enters the window exactly once → added once. O(n) total."
      }
    ]
  },
  {
    "id": "hashmaps",
    "name": "Hash Maps",
    "lessonLink": "/lesson/hashmaps",
    "cards": [
      {
        "question": "What is the typical time complexity of a hash map lookup by key?",
        "tip": "Hash → index → direct access. Average O(1) lookup."
      },
      {
        "question": "In the Two Sum problem, what do we store in the hash map?",
        "tip": "Map: value → index. Need = target - current; check if need is in map."
      },
      {
        "question": "Why use a hash map for Two Sum instead of checking every pair?",
        "tip": "Trade space for time: O(n) pass + O(1) lookup = O(n) total."
      },
      {
        "question": "What is 'need' in the Two Sum algorithm when we're at index i?",
        "tip": "need = target - arr[i]. If need is in map, we have a pair."
      },
      {
        "question": "When do we add an element to the map in Two Sum?",
        "tip": "Check first, then put. So we don't use the same index twice."
      },
      {
        "question": "What is a frequency counter pattern?",
        "tip": "One pass: increment count for each element. Then O(1) queries."
      },
      {
        "question": "For Two Sum, in what order do we check and insert?",
        "tip": "Check for need, then map[arr[i]] = i. Order matters."
      },
      {
        "question": "What is the space complexity of the Two Sum hash map solution?",
        "tip": "Map can hold up to n entries → O(n) space."
      },
      {
        "question": "Can we use a set instead of a map for Two Sum if we only need to know whether a pair exists?",
        "tip": "Set = values only. Map = value → index when we need to return indices."
      },
      {
        "question": "Why might we use a map instead of an array for character frequency?",
        "tip": "Sparse or non-integer keys → map. Dense small integers → array OK."
      },
      {
        "question": "For array [2,7,11,15] and target 9, after processing index 0 (value 2), what is in the map?",
        "tip": "Check need=7, not in map, then put (2, 0)."
      },
      {
        "question": "When we find that 'need' is in the map, what do we return?",
        "tip": "Return [index of complement, current index] = [map.get(need), i]."
      },
      {
        "question": "What if the problem allows the same element to be used twice (e.g. [3,3], target 6)?",
        "tip": "Check then insert ensures the complement index is always before i."
      },
      {
        "question": "How does the frequency counter help in 'valid anagram' (same characters, same counts)?",
        "tip": "One map: add for s1, subtract for s2. All zero ⇒ anagram."
      },
      {
        "question": "What is the time complexity of building a frequency map of n elements?",
        "tip": "n elements × O(1) per operation = O(n)."
      }
    ]
  },
  {
    "id": "linkedlists",
    "name": "Linked Lists",
    "lessonLink": "/lesson/linkedlists",
    "cards": [
      {
        "question": "In the three-pointer in-place reversal, what does 'prev' represent?",
        "tip": "prev = head of reversed part. We extend it by pointing curr back to prev."
      },
      {
        "question": "Why do we save 'next = curr.next' before changing curr.next?",
        "tip": "Break, reverse, advance. Save next first or we lose the list."
      },
      {
        "question": "What is the space complexity of the iterative three-pointer reversal?",
        "tip": "Three pointers = O(1). Recursive version would be O(n) stack."
      },
      {
        "question": "How do we find the middle node of a linked list with the fast & slow pattern?",
        "tip": "Fast 2× speed → when fast at end, slow at middle."
      },
      {
        "question": "When does the reversal loop terminate?",
        "tip": "curr === null ⇒ we're done. Return prev as new head."
      },
      {
        "question": "After one iteration (prev=1, curr=2, next=3), what is the state of the link from node 1?",
        "tip": "After first step: 1→null, 2→1. Reversed so far: 2→1."
      },
      {
        "question": "What do we return at the end of the reversal?",
        "tip": "Loop ends when curr = null. New head = prev."
      },
      {
        "question": "For a list with one node, what does the reversal algorithm return?",
        "tip": "One node: next=null, we set its next to null, return it. Same head."
      },
      {
        "question": "In fast & slow for middle, what if the list has even length (e.g. 4 nodes)?",
        "tip": "Even length: slow at first middle. Fast at end ⇒ slow at index n/2 - 1 (0-based)."
      },
      {
        "question": "Why is the recursive reversal O(n) space?",
        "tip": "Recursion depth = list length → O(n) stack space."
      },
      {
        "question": "In the step 'curr.next = prev', what happens to the link from curr to the rest of the list?",
        "tip": "curr.next is overwritten. next variable holds the rest."
      },
      {
        "question": "For list 1→2→3→null, after the reversal loop, what is the new head and what is the link structure?",
        "tip": "Original tail becomes new head. 3→2→1→null."
      },
      {
        "question": "Why can't we do 'curr.next = prev' before saving next?",
        "tip": "Save next first. Always: next = curr.next, then curr.next = prev, then advance."
      },
      {
        "question": "What is the time complexity of the iterative reversal?",
        "tip": "One pass over n nodes, O(1) per node → O(n)."
      },
      {
        "question": "In fast & slow, how do we detect that fast has reached the end?",
        "tip": "Stop when fast is null or fast.next is null. Covers odd and even length."
      }
    ]
  },
  {
    "id": "stacks",
    "name": "Stacks",
    "lessonLink": "/lesson/stacks",
    "cards": [
      {
        "question": "What does LIFO mean in the context of a stack?",
        "tip": "LIFO = last in, first out. Top of stack = most recent."
      },
      {
        "question": "Which operations does a stack typically support?",
        "tip": "Push, pop, peek. All at the top only."
      },
      {
        "question": "In the Next Greater Element problem, what do we store in the stack?",
        "tip": "Stack of indices. Pop when current value is greater than arr[stack top]."
      },
      {
        "question": "What order do we process the array in for Next Greater Element?",
        "tip": "Left to right. Current element 'resolves' smaller elements on the stack."
      },
      {
        "question": "After the main loop in Next Greater Element, what about indices still in the stack?",
        "tip": "Left in stack ⇒ no element to the right was bigger ⇒ -1."
      },
      {
        "question": "What invariant does the monotonic stack maintain in Next Greater Element?",
        "tip": "Monotonic decreasing: bottom ≥ top. Current > top ⇒ pop."
      },
      {
        "question": "Why is the Next Greater Element algorithm O(n) time?",
        "tip": "Each index: one push, at most one pop → 2n operations → O(n)."
      },
      {
        "question": "For Valid Parentheses, when do we pop from the stack?",
        "tip": "Closing bracket ⇒ must match stack top ⇒ pop. Mismatch or empty ⇒ invalid."
      },
      {
        "question": "What does 'peek' do on a stack?",
        "tip": "Peek = look at top. Pop = remove and return top."
      },
      {
        "question": "In a monotonic decreasing stack for NGE, when we pop index j and set result[j] = arr[i], why is arr[i] the next greater for j?",
        "tip": "First time we see a larger value to the right of j → that's NGE for j."
      },
      {
        "question": "For array [2,1,5,1,3,2], what is the Next Greater Element for index 0 (value 2)?",
        "tip": "Scan right from index 0: 1 < 2, 5 > 2 ⇒ answer 5."
      },
      {
        "question": "After processing index i in NGE, what do we do with i?",
        "tip": "Pop those we've resolved, then push current index."
      },
      {
        "question": "How would you adapt the same stack idea for 'Next Smaller Element'?",
        "tip": "Next smaller ⇒ monotonic increasing stack. Pop when current < top."
      },
      {
        "question": "Why do we store indices in the stack instead of values?",
        "tip": "Index → we can get value (arr[i]) and set result[index]."
      },
      {
        "question": "What is the space complexity of the Next Greater Element algorithm?",
        "tip": "Result array O(n). Stack worst case O(n) (all indices pushed)."
      }
    ]
  }
]